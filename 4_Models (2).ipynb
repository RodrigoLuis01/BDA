{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a40599-24f7-4282-bb19-342dca17ea0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n🔍 Evaluating Logistic Regression...\nLogistic Regression - Overall F1: 0.9462\nLogistic Regression - Minority class -> F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n\n🔍 Evaluating Random Forest...\nRandom Forest - Overall F1: 0.9462\nRandom Forest - Minority class -> F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n\n🔍 Evaluating Decision Tree...\nDecision Tree - Overall F1: 0.9462\nDecision Tree - Minority class -> F1: 0.0000, Precision: 0.0000, Recall: 0.0000\n\n🔍 Evaluating GBT...\nGBT - Overall F1: 0.3050\nGBT - Minority class -> F1: 0.0842, Precision: 0.0439, Recall: 0.9997\n\n📊 Final model comparison (sorted by F1-score):\nGBT                  ➤  F1-score: 0.0842\nLogistic Regression  ➤  F1-score: 0.0000\nRandom Forest        ➤  F1-score: 0.0000\nDecision Tree        ➤  F1-score: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegressionModel, \n",
    "    RandomForestClassificationModel, \n",
    "    DecisionTreeClassificationModel, \n",
    "    GBTClassificationModel\n",
    ")\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load slicing model and test set\n",
    "slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "test_ready   = spark.read.format(\"delta\").load(\"/FileStore/data/test_ready\")\n",
    "test_topk    = slicer_model.transform(test_ready)\n",
    "\n",
    "# Setup evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "# Model paths\n",
    "model_paths = {\n",
    "    \"Logistic Regression\": \"/FileStore/models/lr_top10_model_grid\",\n",
    "    \"Random Forest\": \"/FileStore/models/rf_top10_model_grid\",\n",
    "    \"Decision Tree\": \"/FileStore/models/dt_top10_model_grid\",\n",
    "    \"GBT\": \"/FileStore/models/gbt_top10_weighted_model\"\n",
    "}\n",
    "\n",
    "# Corresponding model classes\n",
    "model_classes = {\n",
    "    \"Logistic Regression\": LogisticRegressionModel,\n",
    "    \"Random Forest\": RandomForestClassificationModel,\n",
    "    \"Decision Tree\": DecisionTreeClassificationModel,\n",
    "    \"GBT\": GBTClassificationModel\n",
    "}\n",
    "\n",
    "# Store F1 scores\n",
    "f1_scores = []\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, path in model_paths.items():\n",
    "    print(f\"\\n🔍 Evaluating {model_name}...\")\n",
    "\n",
    "    model_class = model_classes[model_name]\n",
    "    model = model_class.load(path)\n",
    "    \n",
    "    preds = model.transform(test_topk)\n",
    "    \n",
    "    # Overall weighted F1\n",
    "    f1 = evaluator.evaluate(preds)\n",
    "    \n",
    "    # Minority class F1, Precision, Recall\n",
    "    preds_rdd = preds.select(\"prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "    metrics = MulticlassMetrics(preds_rdd)\n",
    "    \n",
    "    f1_minority = metrics.fMeasure(1.0)\n",
    "    precision_minority = metrics.precision(1.0)\n",
    "    recall_minority = metrics.recall(1.0)\n",
    "    \n",
    "    print(f\"{model_name} - Overall F1: {f1:.4f}\")\n",
    "    print(f\"{model_name} - Minority class -> F1: {f1_minority:.4f}, Precision: {precision_minority:.4f}, Recall: {recall_minority:.4f}\")\n",
    "    \n",
    "    f1_scores.append((model_name, f1_minority))\n",
    "\n",
    "# Sort and display results\n",
    "f1_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n📊 Final model comparison (sorted by F1-score):\")\n",
    "for name, score in f1_scores:\n",
    "    print(f\"{name:20} ➤  F1-score: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook4",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}