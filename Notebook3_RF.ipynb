{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2b4d0ca-8720-473a-8e33-2551502c7f4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n✅ Evaluation on validation set:\nF1-score:      0.9462\nPrecision:     0.9308\nRecall:        0.9639\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nConfusion Matrix:\n[[7.13899e+05 2.20000e+01]\n [2.66970e+04 1.00000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load slicing stage (top-10 features)\n",
    "slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "\n",
    "# Load preprocessed datasets (without limit now)\n",
    "train_ready = spark.read.format(\"delta\").load(\"/FileStore/data/train_ready\")\n",
    "val_ready   = spark.read.format(\"delta\").load(\"/FileStore/data/val_ready\")\n",
    "\n",
    "# Apply slicer to keep only top-10 features\n",
    "train_topk = slicer_model.transform(train_ready)\n",
    "val_topk   = slicer_model.transform(val_ready)\n",
    "\n",
    "# Step 1: Separate label == 1 (minority) and others\n",
    "minority_df = train_topk.filter(col(\"label\") == 1)\n",
    "majority_df = train_topk.filter(col(\"label\") != 1)\n",
    "\n",
    "# Step 2: Sample 60% of majority class\n",
    "sampled_majority_df = majority_df.sample(withReplacement=False, fraction=0.1, seed=42)\n",
    "\n",
    "# Step 3: Combine all minority with sampled majority\n",
    "train_reduced = sampled_majority_df.union(minority_df)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features_topK\",\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Grid de parâmetros para RF\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [20, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf.maxBins, [32, 64]) \\\n",
    "    .build()\n",
    "\n",
    "# Avaliador primário (F1)\n",
    "f1_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "# Avaliadores adicionais\n",
    "precision_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "recall_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "# TrainValidationSplit\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=rf,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=f1_evaluator,\n",
    "    trainRatio=0.7,\n",
    "    parallelism=1\n",
    ")\n",
    "\n",
    "# Treinar modelo\n",
    "tvs_model = tvs.fit(train_reduced)\n",
    "\n",
    "# Avaliação no conjunto de validação externo\n",
    "val_preds = tvs_model.transform(val_topk)\n",
    "\n",
    "f1_score = f1_evaluator.evaluate(val_preds)\n",
    "precision = precision_evaluator.evaluate(val_preds)\n",
    "recall = recall_evaluator.evaluate(val_preds)\n",
    "\n",
    "print(\"\\n✅ Evaluation on validation set:\")\n",
    "print(f\"F1-score:      {f1_score:.4f}\")\n",
    "print(f\"Precision:     {precision:.4f}\")\n",
    "print(f\"Recall:        {recall:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "predictionAndLabels = val_preds.select(\"prediction\", \"label\") \\\n",
    "                               .rdd.map(lambda row: (float(row.prediction), float(row.label)))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "# Guardar melhor modelo\n",
    "tvs_model.bestModel.write().overwrite().save(\"/FileStore/models/rf_top10_model\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook3_RF",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}