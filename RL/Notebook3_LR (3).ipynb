{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df8ae09-6f94-4dd9-ad7b-21d10b51190a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nDistribuição após balanceamento:\n+-----+-------+\n|label|  count|\n+-----+-------+\n|  0.0|2699450|\n|  1.0| 125441|\n+-----+-------+\n\n\n🔍 Threshold Search (para F1 classe 1):\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 0.05 | F1 (classe 1): 0.0696\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/spark/python/pyspark/sql/context.py:165: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 0.10 | F1 (classe 1): 0.0696\nThreshold = 0.15 | F1 (classe 1): 0.0696\nThreshold = 0.20 | F1 (classe 1): 0.0696\nThreshold = 0.25 | F1 (classe 1): 0.0696\nThreshold = 0.30 | F1 (classe 1): 0.0696\nThreshold = 0.35 | F1 (classe 1): 0.0696\nThreshold = 0.40 | F1 (classe 1): 0.0696\nThreshold = 0.45 | F1 (classe 1): 0.0702\nThreshold = 0.50 | F1 (classe 1): 0.0727\nThreshold = 0.55 | F1 (classe 1): 0.0645\nThreshold = 0.60 | F1 (classe 1): 0.0282\nThreshold = 0.65 | F1 (classe 1): 0.0095\nThreshold = 0.70 | F1 (classe 1): 0.0025\nThreshold = 0.75 | F1 (classe 1): 0.0004\nThreshold = 0.80 | F1 (classe 1): 0.0001\nThreshold = 0.85 | F1 (classe 1): 0.0000\nThreshold = 0.90 | F1 (classe 1): 0.0000\n\n✅ Melhor Threshold Encontrado: 0.50 com F1 da classe 1 = 0.0727\n\nConfusion Matrix (com melhor threshold):\n[[371688. 342233.]\n [ 12772.  13926.]]\n\n🎯 Métricas finais com melhor threshold:\nPrecision classe 1: 0.0391\nRecall classe 1:    0.5216\nF1 classe 1:        0.0727\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Carregar modelos e dados\n",
    "slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "train_ready = spark.read.format(\"delta\").load(\"/FileStore/data/train_ready\")\n",
    "val_ready = spark.read.format(\"delta\").load(\"/FileStore/data/val_ready\")\n",
    "\n",
    "# Aplicar slicer\n",
    "train_topk = slicer_model.transform(train_ready)\n",
    "val_topk = slicer_model.transform(val_ready)\n",
    "\n",
    "# Balanceamento leve\n",
    "minority_df = train_topk.filter(col(\"label\") == 1)\n",
    "majority_df = train_topk.filter(col(\"label\") != 1)\n",
    "train_balanced = majority_df.sample(False, 0.8, seed=42).union(minority_df)\n",
    "\n",
    "# Verificar distribuição após balanceamento\n",
    "print(\"\\nDistribuição após balanceamento:\")\n",
    "train_balanced.groupBy(\"label\").count().show()\n",
    "\n",
    "# Calcular pesos por classe\n",
    "label_counts = train_balanced.groupBy(\"label\").count().collect()\n",
    "label_dict = {row[\"label\"]: row[\"count\"] for row in label_counts}\n",
    "total = sum(label_dict.values())\n",
    "class_weights = {label: total / count for label, count in label_dict.items()}\n",
    "\n",
    "# Criar UDF para aplicar pesos\n",
    "def get_weight(label):\n",
    "    return float(class_weights[label])\n",
    "\n",
    "weight_udf = F.udf(get_weight, DoubleType())\n",
    "\n",
    "# Adicionar coluna de pesos\n",
    "train_weighted = train_balanced.withColumn(\"classWeightCol\", weight_udf(col(\"label\")))\n",
    "\n",
    "# ➡️ Substituir RF por Logistic Regression\n",
    "lr = LogisticRegression(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    weightCol=\"classWeightCol\",  # muito importante!\n",
    "    maxIter=100,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.0  # Ridge (L2), pode ajustar\n",
    ")\n",
    "\n",
    "# Treinar modelo\n",
    "model = lr.fit(train_weighted)\n",
    "\n",
    "# Inferência no conjunto de validação\n",
    "val_preds = model.transform(val_topk)\n",
    "\n",
    "# Função para aplicar threshold\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def apply_threshold(df, threshold):\n",
    "    predict_udf = udf(lambda prob: float(1.0) if prob[1] > threshold else float(0.0), DoubleType())\n",
    "    return df.withColumn(\"adjusted_prediction\", predict_udf(col(\"probability\")))\n",
    "\n",
    "# GridSearch pelo melhor threshold\n",
    "best_f1 = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\n🔍 Threshold Search (para F1 classe 1):\")\n",
    "for t in [x / 100.0 for x in range(5, 95, 5)]:\n",
    "    adjusted_df = apply_threshold(val_preds, t)\n",
    "    rdd = adjusted_df.select(\"adjusted_prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "    metrics = MulticlassMetrics(rdd)\n",
    "    f1_class1 = metrics.fMeasure(1.0)\n",
    "    print(f\"Threshold = {t:.2f} | F1 (classe 1): {f1_class1:.4f}\")\n",
    "    if f1_class1 > best_f1:\n",
    "        best_f1 = f1_class1\n",
    "        best_threshold = t\n",
    "\n",
    "print(f\"\\n✅ Melhor Threshold Encontrado: {best_threshold:.2f} com F1 da classe 1 = {best_f1:.4f}\")\n",
    "\n",
    "# Aplicar melhor threshold\n",
    "val_preds_adjusted = apply_threshold(val_preds, best_threshold)\n",
    "\n",
    "# Avaliação final\n",
    "final_rdd = val_preds_adjusted.select(\"adjusted_prediction\", \"label\").rdd.map(lambda r: (float(r[0]), float(r[1])))\n",
    "metrics = MulticlassMetrics(final_rdd)\n",
    "\n",
    "print(\"\\nConfusion Matrix (com melhor threshold):\")\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "\n",
    "print(\"\\n🎯 Métricas finais com melhor threshold:\")\n",
    "print(f\"Precision classe 1: {metrics.precision(1.0):.4f}\")\n",
    "print(f\"Recall classe 1:    {metrics.recall(1.0):.4f}\")\n",
    "print(f\"F1 classe 1:        {metrics.fMeasure(1.0):.4f}\")\n",
    "\n",
    "# Guardar modelo\n",
    "model.write().overwrite().save(\"/FileStore/models/lr_top10_weighted_model\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook3_LR",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}