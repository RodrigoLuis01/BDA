{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b22023e-baee-44a2-ae28-14c579c1abf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6834448\ttotal: 201ms\tremaining: 40s\n50:\tlearn: 0.6260196\ttotal: 7.75s\tremaining: 22.6s\n100:\tlearn: 0.6251217\ttotal: 14.8s\tremaining: 14.5s\n150:\tlearn: 0.6244234\ttotal: 22s\tremaining: 7.14s\n199:\tlearn: 0.6237874\ttotal: 29s\tremaining: 0us\n\n✅ Classification Report on Validation Set:\n              precision    recall  f1-score   support\n\n         0.0       1.00      0.19      0.32    713921\n         1.0       0.04      0.99      0.08     26698\n\n    accuracy                           0.22    740619\n   macro avg       0.52      0.59      0.20    740619\nweighted avg       0.96      0.22      0.31    740619\n\nConfusion Matrix:\n[[137857 576064]\n [   259  26439]]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import DenseVector, SparseVector\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Load slicing stage (top-10 features)\n",
    "slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "\n",
    "# Load preprocessed datasets (limit for speed)\n",
    "train_ready = spark.read.format(\"delta\").load(\"/FileStore/data/train_ready\")\n",
    "val_ready   = spark.read.format(\"delta\").load(\"/FileStore/data/val_ready\")\n",
    "\n",
    "# Apply slicer to keep only top-10 features\n",
    "train_topk = slicer_model.transform(train_ready)\n",
    "val_topk   = slicer_model.transform(val_ready)\n",
    "\n",
    "# ========== Light undersampling ==========\n",
    "minority_df = train_topk.filter(col(\"label\") == 1)\n",
    "majority_df = train_topk.filter(col(\"label\") != 1)\n",
    "train_balanced = majority_df.sample(False, 0.1, seed=42).union(minority_df)\n",
    "\n",
    "# ========== Converter VectorUDT para lista de floats ==========\n",
    "def vector_to_array(v):\n",
    "    if isinstance(v, DenseVector) or isinstance(v, SparseVector):\n",
    "        return v.toArray().tolist()\n",
    "    else:\n",
    "        return v\n",
    "\n",
    "vector_to_array_udf = udf(vector_to_array, ArrayType(DoubleType()))\n",
    "\n",
    "train_array = train_balanced.withColumn(\"features_array\", vector_to_array_udf(\"features_topK\"))\n",
    "val_array = val_topk.withColumn(\"features_array\", vector_to_array_udf(\"features_topK\"))\n",
    "\n",
    "# ========== Convert to Pandas ==========\n",
    "train_pd = train_array.select(\"features_array\", \"label\").toPandas()\n",
    "val_pd   = val_array.select(\"features_array\", \"label\").toPandas()\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = pd.DataFrame(train_pd[\"features_array\"].tolist())\n",
    "y_train = train_pd[\"label\"]\n",
    "\n",
    "X_val = pd.DataFrame(val_pd[\"features_array\"].tolist())\n",
    "y_val = val_pd[\"label\"]\n",
    "\n",
    "# ========== Class Weights ==========\n",
    "label_counts = y_train.value_counts().to_dict()\n",
    "total = sum(label_counts.values())\n",
    "class_weights = {label: total / count for label, count in label_counts.items()}\n",
    "\n",
    "# Sort by label index to match CatBoost order\n",
    "class_weights_list = [class_weights[i] for i in sorted(class_weights)]\n",
    "\n",
    "# ========== Train CatBoost ==========\n",
    "model = CatBoostClassifier(\n",
    "    iterations=200,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    loss_function='MultiClass',\n",
    "    class_weights=class_weights_list,\n",
    "    verbose=50,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ========== Evaluate ==========\n",
    "y_pred = model.predict(X_val)\n",
    "print(\"\\n✅ Classification Report on Validation Set:\")\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_pred))\n",
    "\n",
    "# 1. Certifica-te que o diretório existe\n",
    "import os\n",
    "os.makedirs(\"/dbfs/FileStore/models/\", exist_ok=True)\n",
    "\n",
    "# 2. Salva o modelo\n",
    "model.save_model(\"/dbfs/FileStore/models/cb_top10_model_weighted.cbm\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook3_CB",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}