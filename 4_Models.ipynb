{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a40599-24f7-4282-bb19-342dca17ea0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score no conjunto de teste: 0.9462\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import (\n",
    "    LogisticRegressionModel, \n",
    "    RandomForestClassificationModel, \n",
    "    DecisionTreeClassificationModel, \n",
    "    GBTClassificationModel\n",
    ")\n",
    "\n",
    "# Load slicing model and test set\n",
    "slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "test_ready   = spark.read.format(\"delta\").load(\"/FileStore/data/test_ready\")\n",
    "test_topk    = slicer_model.transform(test_ready)\n",
    "\n",
    "# Setup evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "# Model paths\n",
    "model_paths = {\n",
    "    \"Logistic Regression\": \"/FileStore/models/lr_top10_model_grid\",\n",
    "    \"Random Forest\": \"/FileStore/models/rf_top10_model_grid\",\n",
    "    \"Decision Tree\": \"/FileStore/models/dt_top10_model_grid\",\n",
    "    \"GBT\": \"/FileStore/models/gbt_top10_model_grid\"\n",
    "}\n",
    "\n",
    "# Corresponding model classes\n",
    "model_classes = {\n",
    "    \"Logistic Regression\": LogisticRegressionModel,\n",
    "    \"Random Forest\": RandomForestClassificationModel,\n",
    "    \"Decision Tree\": DecisionTreeClassificationModel,\n",
    "    \"GBT\": GBTClassificationModel\n",
    "}\n",
    "\n",
    "# Store F1 scores\n",
    "f1_scores = []\n",
    "\n",
    "# Evaluate each model\n",
    "for model_name, path in model_paths.items():\n",
    "    print(f\"\\nüîç Evaluating {model_name}...\")\n",
    "\n",
    "    model_class = model_classes[model_name]\n",
    "    model = model_class.load(path)\n",
    "    \n",
    "    preds = model.transform(test_topk)\n",
    "    f1 = evaluator.evaluate(preds)\n",
    "    \n",
    "    print(f\"{model_name} - F1-score on test set: {f1:.4f}\")\n",
    "    \n",
    "    f1_scores.append((model_name, f1))\n",
    "\n",
    "# Sort and display results\n",
    "f1_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nüìä Final model comparison (sorted by F1-score):\")\n",
    "for name, score in f1_scores:\n",
    "    print(f\"{name:20} ‚û§  F1-score: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook4",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}