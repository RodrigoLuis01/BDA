{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6bd9e23-94be-4112-9380-7a5847c036a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, hour, dayofweek, dayofmonth, month, when, \n",
    "    count, approx_count_distinct, unix_timestamp, sum as _sum, \n",
    "    lag, avg, isnan, to_date, lit, countDistinct\n",
    ")\n",
    "\n",
    "from pyspark.sql import Window, functions as F\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, Imputer, SQLTransformer, FeatureHasher\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation      import BinaryClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6f23e5c-bd6e-4f45-8f75-18e9d09bf065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = spark.read.option(\"header\", True).option(\"inferSchema\", True) \\\n",
    "    .csv(\"/FileStore/tables/financial_fraud_detection_dataset_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb444fb0-82be-475e-aa8c-1bcab32f011b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/tables/financial_fraud_detection_dataset.csv</td><td>financial_fraud_detection_dataset.csv</td><td>796050204</td><td>1747235797000</td></tr><tr><td>dbfs:/FileStore/tables/financial_fraud_detection_dataset_1.csv</td><td>financial_fraud_detection_dataset_1.csv</td><td>796050204</td><td>1748467424000</td></tr><tr><td>dbfs:/FileStore/tables/joao-1.dbc</td><td>joao-1.dbc</td><td>36736</td><td>1747732766000</td></tr><tr><td>dbfs:/FileStore/tables/joao.dbc</td><td>joao.dbc</td><td>37814</td><td>1747732619000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/tables/financial_fraud_detection_dataset.csv",
         "financial_fraud_detection_dataset.csv",
         796050204,
         1747235797000
        ],
        [
         "dbfs:/FileStore/tables/financial_fraud_detection_dataset_1.csv",
         "financial_fraud_detection_dataset_1.csv",
         796050204,
         1748467424000
        ],
        [
         "dbfs:/FileStore/tables/joao-1.dbc",
         "joao-1.dbc",
         36736,
         1747732766000
        ],
        [
         "dbfs:/FileStore/tables/joao.dbc",
         "joao.dbc",
         37814,
         1747732619000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"/FileStore/tables/\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf2bd5ea-4b61-4afc-a33b-c18ae60b19cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Initial Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "758ecdff-8ee9-4f16-922a-e1a86a5365f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>transaction_id</th><th>timestamp</th><th>sender_account</th><th>receiver_account</th><th>amount</th><th>transaction_type</th><th>merchant_category</th><th>location</th><th>device_used</th><th>is_fraud</th><th>fraud_type</th><th>time_since_last_transaction</th><th>spending_deviation_score</th><th>velocity_score</th><th>geo_anomaly_score</th><th>payment_channel</th><th>ip_address</th><th>device_hash</th></tr></thead><tbody><tr><td>T100000</td><td>2023-08-22T09:22:43.516+0000</td><td>ACC877572</td><td>ACC388389</td><td>343.78</td><td>withdrawal</td><td>utilities</td><td>Tokyo</td><td>mobile</td><td>false</td><td>null</td><td>null</td><td>-0.21</td><td>3</td><td>0.22</td><td>card</td><td>13.101.214.112</td><td>D8536477</td></tr><tr><td>T100001</td><td>2023-08-04T01:58:02.606+0000</td><td>ACC895667</td><td>ACC944962</td><td>419.65</td><td>withdrawal</td><td>online</td><td>Toronto</td><td>atm</td><td>false</td><td>null</td><td>null</td><td>-0.14</td><td>7</td><td>0.96</td><td>ACH</td><td>172.52.47.194</td><td>D2622631</td></tr><tr><td>T100002</td><td>2023-05-12T11:39:33.742+0000</td><td>ACC733052</td><td>ACC377370</td><td>2773.86</td><td>deposit</td><td>other</td><td>London</td><td>pos</td><td>false</td><td>null</td><td>null</td><td>-1.78</td><td>20</td><td>0.89</td><td>card</td><td>185.98.35.23</td><td>D4823498</td></tr><tr><td>T100003</td><td>2023-10-10T06:04:43.195+0000</td><td>ACC996865</td><td>ACC344098</td><td>1666.22</td><td>deposit</td><td>online</td><td>Sydney</td><td>pos</td><td>false</td><td>null</td><td>null</td><td>-0.6</td><td>6</td><td>0.37</td><td>wire_transfer</td><td>107.136.36.87</td><td>D9961380</td></tr><tr><td>T100004</td><td>2023-09-24T08:09:02.700+0000</td><td>ACC584714</td><td>ACC497887</td><td>24.43</td><td>transfer</td><td>utilities</td><td>Toronto</td><td>mobile</td><td>false</td><td>null</td><td>null</td><td>0.79</td><td>13</td><td>0.27</td><td>ACH</td><td>108.161.108.255</td><td>D7637601</td></tr><tr><td>T100005</td><td>2023-11-20T17:49:27.940+0000</td><td>ACC581141</td><td>ACC249811</td><td>58.77</td><td>transfer</td><td>entertainment</td><td>Berlin</td><td>atm</td><td>false</td><td>null</td><td>null</td><td>-1.63</td><td>19</td><td>0.4</td><td>UPI</td><td>112.70.252.46</td><td>D1790481</td></tr><tr><td>T100006</td><td>2023-11-11T11:15:41.359+0000</td><td>ACC757924</td><td>ACC267753</td><td>59.51</td><td>payment</td><td>travel</td><td>Dubai</td><td>pos</td><td>false</td><td>null</td><td>null</td><td>-2.62</td><td>1</td><td>0.68</td><td>UPI</td><td>58.136.174.57</td><td>D5924115</td></tr><tr><td>T100007</td><td>2023-06-09T06:32:36.945+0000</td><td>ACC103402</td><td>ACC857168</td><td>29.79</td><td>transfer</td><td>entertainment</td><td>London</td><td>atm</td><td>false</td><td>null</td><td>null</td><td>-0.48</td><td>5</td><td>0.37</td><td>ACH</td><td>82.0.165.250</td><td>D1326765</td></tr><tr><td>T100008</td><td>2023-02-11T06:57:40.585+0000</td><td>ACC972064</td><td>ACC945964</td><td>16.0</td><td>transfer</td><td>utilities</td><td>New York</td><td>web</td><td>false</td><td>null</td><td>null</td><td>0.99</td><td>3</td><td>0.98</td><td>card</td><td>64.65.243.84</td><td>D5446912</td></tr><tr><td>T100009</td><td>2023-07-13T09:25:20.560+0000</td><td>ACC543692</td><td>ACC322086</td><td>203.05</td><td>withdrawal</td><td>grocery</td><td>Dubai</td><td>atm</td><td>false</td><td>null</td><td>null</td><td>0.59</td><td>17</td><td>0.45</td><td>UPI</td><td>126.115.32.173</td><td>D1352896</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "T100000",
         "2023-08-22T09:22:43.516+0000",
         "ACC877572",
         "ACC388389",
         343.78,
         "withdrawal",
         "utilities",
         "Tokyo",
         "mobile",
         false,
         null,
         null,
         -0.21,
         3,
         0.22,
         "card",
         "13.101.214.112",
         "D8536477"
        ],
        [
         "T100001",
         "2023-08-04T01:58:02.606+0000",
         "ACC895667",
         "ACC944962",
         419.65,
         "withdrawal",
         "online",
         "Toronto",
         "atm",
         false,
         null,
         null,
         -0.14,
         7,
         0.96,
         "ACH",
         "172.52.47.194",
         "D2622631"
        ],
        [
         "T100002",
         "2023-05-12T11:39:33.742+0000",
         "ACC733052",
         "ACC377370",
         2773.86,
         "deposit",
         "other",
         "London",
         "pos",
         false,
         null,
         null,
         -1.78,
         20,
         0.89,
         "card",
         "185.98.35.23",
         "D4823498"
        ],
        [
         "T100003",
         "2023-10-10T06:04:43.195+0000",
         "ACC996865",
         "ACC344098",
         1666.22,
         "deposit",
         "online",
         "Sydney",
         "pos",
         false,
         null,
         null,
         -0.6,
         6,
         0.37,
         "wire_transfer",
         "107.136.36.87",
         "D9961380"
        ],
        [
         "T100004",
         "2023-09-24T08:09:02.700+0000",
         "ACC584714",
         "ACC497887",
         24.43,
         "transfer",
         "utilities",
         "Toronto",
         "mobile",
         false,
         null,
         null,
         0.79,
         13,
         0.27,
         "ACH",
         "108.161.108.255",
         "D7637601"
        ],
        [
         "T100005",
         "2023-11-20T17:49:27.940+0000",
         "ACC581141",
         "ACC249811",
         58.77,
         "transfer",
         "entertainment",
         "Berlin",
         "atm",
         false,
         null,
         null,
         -1.63,
         19,
         0.4,
         "UPI",
         "112.70.252.46",
         "D1790481"
        ],
        [
         "T100006",
         "2023-11-11T11:15:41.359+0000",
         "ACC757924",
         "ACC267753",
         59.51,
         "payment",
         "travel",
         "Dubai",
         "pos",
         false,
         null,
         null,
         -2.62,
         1,
         0.68,
         "UPI",
         "58.136.174.57",
         "D5924115"
        ],
        [
         "T100007",
         "2023-06-09T06:32:36.945+0000",
         "ACC103402",
         "ACC857168",
         29.79,
         "transfer",
         "entertainment",
         "London",
         "atm",
         false,
         null,
         null,
         -0.48,
         5,
         0.37,
         "ACH",
         "82.0.165.250",
         "D1326765"
        ],
        [
         "T100008",
         "2023-02-11T06:57:40.585+0000",
         "ACC972064",
         "ACC945964",
         16.0,
         "transfer",
         "utilities",
         "New York",
         "web",
         false,
         null,
         null,
         0.99,
         3,
         0.98,
         "card",
         "64.65.243.84",
         "D5446912"
        ],
        [
         "T100009",
         "2023-07-13T09:25:20.560+0000",
         "ACC543692",
         "ACC322086",
         203.05,
         "withdrawal",
         "grocery",
         "Dubai",
         "atm",
         false,
         null,
         null,
         0.59,
         17,
         0.45,
         "UPI",
         "126.115.32.173",
         "D1352896"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "transaction_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "sender_account",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "receiver_account",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "transaction_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "merchant_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "device_used",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "is_fraud",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "fraud_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "time_since_last_transaction",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "spending_deviation_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "velocity_score",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "geo_anomaly_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "payment_channel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ip_address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "device_hash",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df544bd6-7358-4348-b875-b5d0e3a24f18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Count values of fraud_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2236c8b3-2ad1-446b-8330-f6cb875a217f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n|fraud_type      |count  |\n+----------------+-------+\n|null            |4820447|\n|card_not_present|179553 |\n+----------------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "fraud_type_counts = data.groupBy(\"fraud_type\").count()\n",
    "\n",
    "# Display the results\n",
    "fraud_type_counts.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f2dad28-dcfb-43fa-9cab-53a0abe82393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Drop 'fraud_type' from the dataset to prevent target leakage\n",
    "It is only populated for known frauds and could give away the label to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c71a6de8-fd3c-4f01-bd89-a086b9f07eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_nofraud = data.drop(\"fraud_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dff8748-1723-497b-95a9-bdb774701e9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Count values for time_since_last_transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1524e3ae-2fa9-4d63-92bf-92283263d192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-----+\n|time_since_last_transaction|count|\n+---------------------------+-----+\n|4726.583900918889          |1    |\n|502.8021655611111          |1    |\n|-5569.000236446667         |1    |\n|5529.572787373056          |1    |\n|2457.737092543611          |1    |\n|1760.5630150244444         |1    |\n|685.5268574536111          |1    |\n|5386.103629349167          |1    |\n|-2089.4958554238888        |1    |\n|-1005.4869802922223        |1    |\n|63.23230790805556          |1    |\n|-1662.2153012366668        |1    |\n|203.02668470999998         |1    |\n|351.8206997013889          |1    |\n|-6124.013181173334         |1    |\n|3692.413661105             |1    |\n|1277.0709516263887         |1    |\n|6810.93473166              |1    |\n|1915.671854693889          |1    |\n|5393.090344617777          |1    |\n+---------------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Assuming your DataFrame is named 'data'\n",
    "time_since_last_transaction_count = data.groupBy(\"time_since_last_transaction\").count()\n",
    "\n",
    "# Display the results\n",
    "time_since_last_transaction_count.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "632ea1a6-5983-4e3a-9f21-35c9582d1d4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "No strange values, but there are a lot of missing values, we'll handle it in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d8d879-56c6-4dcf-9b25-4864e55f78bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Duplicates rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bd7e0c7-c6d1-4691-9ebf-828889032058",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate transaction_id’s: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate transaction IDs (should be unique per row)\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "dupes = (data_nofraud\n",
    "         .groupBy(\"transaction_id\")\n",
    "         .agg(count(\"*\").alias(\"cnt\"))\n",
    "         .filter(col(\"cnt\") > 1))\n",
    "\n",
    "dupes_count = dupes.count()\n",
    "print(f\"Duplicate transaction_id’s: {dupes_count:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1b335a0-8948-4da0-882c-3b75e0db4ba8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Since there are no duplicates, we proceed with the same DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df3c0d41-4fef-48ff-aa1d-ed05231bd130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_nodup = data_nofraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b8be0a6-437e-4b15-913f-809a79c31620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Audit & correct column types / obvious quirks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce9c2685-b8d5-47f5-961a-e13a86372185",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa67deea-afd8-4ed7-aeff-1e234ffd013e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row count: 5000000\nroot\n |-- transaction_id: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- sender_account: string (nullable = true)\n |-- receiver_account: string (nullable = true)\n |-- amount: double (nullable = true)\n |-- transaction_type: string (nullable = true)\n |-- merchant_category: string (nullable = true)\n |-- location: string (nullable = true)\n |-- device_used: string (nullable = true)\n |-- is_fraud: boolean (nullable = true)\n |-- time_since_last_transaction: double (nullable = true)\n |-- spending_deviation_score: double (nullable = true)\n |-- velocity_score: integer (nullable = true)\n |-- geo_anomaly_score: double (nullable = true)\n |-- payment_channel: string (nullable = true)\n |-- ip_address: string (nullable = true)\n |-- device_hash: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "print(\"Row count:\", data_nodup.count())\n",
    "data_nodup.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c494a08-05db-45b8-9cd9-8ab9dcd3add9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Which ones should we fix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab5fcec-7944-4918-a21b-38272555c9a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Column           | Issue we might see                                          | How to fix            |\n",
    "| ---------------- | ------------------------------------------------------------ | ---------------------- |\n",
    "| `timestamp`      | stored as **string** in some dumps                           | `to_timestamp`         |\n",
    "| `velocity_score` | `long` but we want **double** to keep everything numeric | `col.cast(\"double\")`   |\n",
    "| `amount`         | negative or 0 values?                                        | `filter(\"amount > 0\")` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79e98a8c-13dc-462e-bc28-151464cdca11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After type fix + sanity filters: 5000000\n+--------------------------+-------+--------------+--------+\n|event_time                |amount |velocity_score|is_fraud|\n+--------------------------+-------+--------------+--------+\n|2023-08-22 09:22:43.516168|343.78 |3.0           |false   |\n|2023-08-04 01:58:02.606711|419.65 |7.0           |false   |\n|2023-05-12 11:39:33.742963|2773.86|20.0          |false   |\n|2023-10-10 06:04:43.195112|1666.22|6.0           |false   |\n|2023-09-24 08:09:02.700162|24.43  |13.0          |false   |\n+--------------------------+-------+--------------+--------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Type correction + basic sanity filters\n",
    "data_typed = (\n",
    "    data_nodup\n",
    "    .withColumn(\"event_time\", to_timestamp(\"timestamp\"))         # convert to timestamp\n",
    "    .withColumn(\"velocity_score\", col(\"velocity_score\").cast(\"double\"))  # ensure consistent numeric type\n",
    ")\n",
    "\n",
    "# Drop rows with nonsensical values (sanity filters)\n",
    "data_sane = (\n",
    "    data_typed\n",
    "    .filter(col(\"amount\") > 0)                     # negative/zero amount is invalid\n",
    "    .filter(col(\"event_time\").isNotNull())         # remove rows with bad timestamps\n",
    ")\n",
    "\n",
    "print(\"After type fix + sanity filters:\", data_sane.count())\n",
    "data_sane.select(\"event_time\", \"amount\", \"velocity_score\", \"is_fraud\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7812b9d-e37f-4891-8390-f175824ca689",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Observation                                                    | What it tells us                                                                          | Implication for next steps                                                                                                                                              |\n",
    "| -------------------------------------------------------------- | ----------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Row count unchanged (5 000 000)** after filters              | All rows had positive `amount`, non-null `timestamp`, and unique `transaction_id`.        | We don’t need extra cleansing; we can proceed straight to feature creation.                                                                                             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66ba420-ca6c-4afd-985f-a954bff38bde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c038e3c-1223-4dc7-a437-4a6959e69591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null / NaN count per column:\n-RECORD 0-----------------------------\n transaction_id              | 0      \n timestamp                   | 0      \n sender_account              | 0      \n receiver_account            | 0      \n amount                      | 0      \n transaction_type            | 0      \n merchant_category           | 0      \n location                    | 0      \n device_used                 | 0      \n is_fraud                    | 0      \n time_since_last_transaction | 896513 \n spending_deviation_score    | 0      \n velocity_score              | 0      \n geo_anomaly_score           | 0      \n payment_channel             | 0      \n ip_address                  | 0      \n device_hash                 | 0      \n event_time                  | 0      \n\n"
     ]
    }
   ],
   "source": [
    "df = data_sane \n",
    "\n",
    "# Identify numeric columns\n",
    "num_cols = [\"amount\", \"time_since_last_transaction\", \"spending_deviation_score\",\n",
    "            \"velocity_score\", \"geo_anomaly_score\"]\n",
    "\n",
    "# Build null + NaN check for each column\n",
    "null_exprs = []\n",
    "for c in df.columns:\n",
    "    if c in num_cols:\n",
    "        # For numeric: check both isNull and isnan\n",
    "        null_exprs.append(_sum((col(c).isNull() | isnan(c)).cast(\"int\")).alias(c))\n",
    "    else:\n",
    "        # For all others: only isNull\n",
    "        null_exprs.append(_sum(col(c).isNull().cast(\"int\")).alias(c))\n",
    "\n",
    "# Run audit\n",
    "null_counts = df.select(null_exprs)\n",
    "print(\"Null / NaN count per column:\")\n",
    "null_counts.show(vertical=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3fde211-167a-4401-a574-f2b752504b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Column                            | Null count                   | Action                                            |\n",
    "| --------------------------------- | ---------------------------- | ------------------------------------------------- |\n",
    "| Everything else                   | `0`                          | ✅ Already clean — no action needed                |\n",
    "| **`time_since_last_transaction`** | `896,513` (≈ 18% of dataset) | Needs **imputation** (don’t drop this much data!) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32552dc9-104c-4167-a107-9e4524ed12a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.1 Impute missing values on time_since_last_transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "446ec58a-b49f-4b3f-9b56-7aaec28453dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Checking for percentage of negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c0c4fb2-2bbb-43ab-8c5c-d0c9431931e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negatives: 2,051,331  (41.03%)\n"
     ]
    }
   ],
   "source": [
    "total = data_sane.count()\n",
    "neg  = data_sane.filter(F.col(\"time_since_last_transaction\") < 0).count()\n",
    "print(f\"Negatives: {neg:,}  ({neg/total:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d32ed14-76db-48f8-ab27-f3df2206c491",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are a significant number of invalid or negative values in `time_since_last_transaction` (41.03%).\n",
    "\n",
    "This conflicts with its expected non-negative nature. Converting these to `NULL` before imputation could exacerbate the existing 18% missing values, risking the loss of a lot of rows. Instead, imputing with the median metric, preserves the data’s natural scale and retains most rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33efbf51-f6aa-49ff-a722-36cec6820926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Fix negatives using abs()\n",
    "data_fix = data_sane.withColumn(\n",
    "    \"time_since_last_tx_tmp\",\n",
    "    F.abs(F.col(\"time_since_last_transaction\"))\n",
    ")\n",
    "\n",
    "# Impute the single column\n",
    "imputer = Imputer(\n",
    "    strategy=\"median\",\n",
    "    inputCols=[\"time_since_last_tx_tmp\"],\n",
    "    outputCols=[\"time_since_last_transaction_imp\"]\n",
    ")\n",
    "\n",
    "# Update your numeric feature list to use the imputed version\n",
    "num_cols = [\"amount\",\n",
    "            \"spending_deviation_score\", \n",
    "            \"velocity_score\",  \n",
    "            \"geo_anomaly_score\",\n",
    "            \"time_since_last_transaction_imp\"]\n",
    "\n",
    "data_imputed = imputer.fit(data_fix).transform(data_fix)\n",
    "\n",
    "# Infering all string-typed columns, excluding label\n",
    "cat_cols = [c for c, t in data_imputed.dtypes\n",
    "            if t == \"string\" and c != \"is_fraud\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17e67843-6bb8-4566-84d6-2500173fe449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Data-quality quick-stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1383c84d-7f6b-4872-8f0f-93e44776c5e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 5.1 Overview of Row Count, Unique Accounts, and Fraud Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "391094aa-d642-4e0c-a730-3cba8ecb4eb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+----------------+----------------+\n|rows   |unique_senders|unique_receivers|fraud_percentage|\n+-------+--------------+----------------+----------------+\n|5000000|874981        |870056          |3.59106         |\n+-------+--------------+----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "dq = (data_imputed\n",
    "      .agg(\n",
    "          F.count('*').alias('rows'),\n",
    "          F.approx_count_distinct('sender_account').alias('unique_senders'),\n",
    "          F.approx_count_distinct('receiver_account').alias('unique_receivers'),\n",
    "          F.avg(F.col('is_fraud').cast('double')).alias('fraud_rate')  # average == prevalence\n",
    "      )\n",
    "      .withColumn('fraud_percentage', F.col('fraud_rate') * 100)\n",
    "      .drop('fraud_rate')\n",
    ")\n",
    "\n",
    "dq.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f128a799-9796-4f4b-a593-34b542139374",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Observation                                                    | What it tells us                                                                          | Implication for next steps                                                                                                                                              |\n",
    "|----------------------------------------------------------------|------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Fraud rate ≈ 3.6 %** (`≈ 1 : 26` ratio)                      | Still imbalanced, but *much* less extreme than real-world credit-card data (often 0.1 %). | • Weighted loss or stratified sampling is *optional* rather than mandatory.<br>• Use **Area-Under-PR** and remember that naïve accuracy will already be \\~96 %.         |\n",
    "| **≈ 875 k unique senders vs ≈ 870 k receivers**                | Very wide account base; median transactions per account will be modest.                   | Behavioural aggregates (rolling counts, sums) will be informative and cheap to compute because each account’s history is relatively small.                              |\n",
    "| **Timestamps span 2023-03-23 → 2023-10-31** (from the glimpse) | Roughly seven months of activity.                                                         | Use a **time-based train/validation/test split** (e.g., train on March-Aug, validate on September, test on October) to mimic real deployment and detect temporal drift. |\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59c8a9db-20cd-41a8-8fd7-919cfd82b84c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 5.2 Summary Statistics for Numeric Columns: Count, Range, Quartiles, and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88f1f6f6-8b62-4964-966c-88bca1e34b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------------+-----------------+------------------+-------------------------------+\n|summary|           amount|spending_deviation_score|   velocity_score| geo_anomaly_score|time_since_last_transaction_imp|\n+-------+-----------------+------------------------+-----------------+------------------+-------------------------------+\n|  count|          5000000|                 5000000|          5000000|           5000000|                        5000000|\n|    min|             0.01|                   -5.26|              1.0|               0.0|           0.001094368055555...|\n|    25%|            26.56|                   -0.68|              6.0|              0.25|              1454.099700473889|\n|    50%|           138.66|                     0.0|             11.0|               0.5|              2566.543702108889|\n|    75%|           503.79|                    0.67|             16.0|              0.75|             3925.8361853491665|\n|    max|          3520.57|                    5.02|             20.0|               1.0|              8777.814181944444|\n| stddev|469.9333110659362|      1.0008069793965741|5.766842438453673|0.2886349350409801|             1875.9850900887013|\n+-------+-----------------+------------------------+-----------------+------------------+-------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data_imputed.select(num_cols).summary(\"count\",\"min\",\"25%\",\"50%\",\"75%\",\"max\",\"stddev\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "830bd7f4-fe4d-4a06-9f4b-8193a38e92af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Feature                               | Status               | Distribution Shape                                           | Recommendation                                                                                       |\n",
    "| ------------------------------------- | -------------------- | ------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------- |\n",
    "| **`amount`**                          | ✅ Clean              | Highly skewed: min 0.01, median 138, max 3 520, stddev \\~470 | Consider applying `log1p(amount)` in the pipeline (helps linear models, scaling)                     |\n",
    "| **`spending_deviation_score`**        | ✅ Already normalized | Centered at 0, range ≈ \\[−5.26, +5.02], std ≈ 1              | Leave as-is. Already a Z-score.                                                                      |\n",
    "| **`velocity_score`**                  | ✅ Integer bands      | Median 11, range \\[1, 20], evenly spaced                     | Could treat as numeric or one-hot encoded categorical, depending on model.                           |\n",
    "| **`geo_anomaly_score`**               | ✅ Normalized         | Range \\[0, 1], median 0.5                                    | Leave as-is. No transformation needed.                                                               |\n",
    "| **`time_since_last_transaction_imp`** | ✅ Fixed and imputed  | Skewed: median ≈ 2 567, max ≈ 8 778, stddev ≈ 1 876          | Optional: apply `log1p` if you plan to use distance-based or linear models. Otherwise just scale it. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644d365f-7183-4d23-b929-10a3c337ec24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 5.3 Cardinality Check: Distinct Value Counts for Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29d138ee-405b-4048-8ebb-202df90b182e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: [('transaction_id', 5000000),\n ('sender_account', 896513),\n ('receiver_account', 896639),\n ('transaction_type', 4),\n ('merchant_category', 8),\n ('location', 8),\n ('device_used', 4),\n ('payment_channel', 4),\n ('ip_address', 4997068),\n ('device_hash', 3835723)]"
     ]
    }
   ],
   "source": [
    "[(c, data_imputed.select(c).distinct().count()) for c in cat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03855c0-1a20-45bd-89c6-5d713f4207a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "| Column              | Distinct count                | Take-away                               | What we'll do later                                                                                                                                    |\n",
    "| ------------------- | ----------------------------- | --------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `transaction_id`    | **5 000 000** (one per row)   | Pure primary key – no predictive signal | **Drop** from the feature set; keep only as an identifier.                                                                                              |\n",
    "| `ip_address`        | **4 997 068** (≈ one per row) | Near-unique; sparse, privacy-sensitive  | • Either drop **or** hash/bucket into /24 subnets.<br>• Aggregate features (e.g. past-fraud rate per /24) if you have a history table.                  |\n",
    "| `device_hash`       | **3 835 723**                 | Pseudonymised device ID; huge           | Same tactics as `ip_address`: hash trick or frequency-encode.                                                                                           |\n",
    "| `sender_account`    | **896 513**                   | High cardinality but **re-occurs**      | Good candidate for:<br>• frequency encoding (log(#tx))<br>• target / leave-one-out encoding<br>• rolling behavioural aggregates (tx count, avg amount). |\n",
    "| `receiver_account`  | **896 639**                   | Same reasoning as sender                | Same set of encodings / aggregates as above.                                                                                                            |\n",
    "| `transaction_type`  | **4**                         | Low cardinality                         | Straightforward `StringIndexer` → `OneHotEncoder`.                                                                                                      |\n",
    "| `merchant_category` | **8**                         | Low                                     | One-hot.                                                                                                                                                |\n",
    "| `location`          | **8**                         | Low                                     | One-hot.                                                                                                                                                |\n",
    "| `device_used`       | **4**                         | Low                                     | One-hot.                                                                                                                                                |\n",
    "| `payment_channel`   | **4**                         | Low                                     | One-hot.                                                                                                                                                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02aeecab-f815-42cb-bc79-9b5558b95315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Feature enrichment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4975eef5-ce39-4717-b915-1eacfc9d9f00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.1 Time-derived columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da130e6-a85c-44f3-851e-d0a88e4c677f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_feat = (data_imputed\n",
    "  .withColumn(\"hour_of_day\",  hour(\"event_time\"))\n",
    "  .withColumn(\"day_of_week\",  dayofweek(\"event_time\"))   # 1=Sun … 7=Sat\n",
    "  .withColumn(\"is_weekend\",   when(dayofweek(\"event_time\").isin(1,7), 1).otherwise(0))\n",
    "  .withColumn(\"month\",        month(\"event_time\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b81d56b-a8dc-44a6-98d2-ebf8643e51f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 6.2 Rolling behavioural metrics\n",
    "\n",
    "| New col.      | Granularity | Typical signal in fraud                     |\n",
    "| ------------- | ----------- | ------------------------------------------- |\n",
    "| `hour_of_day` | 0-23        | Late-night spikes, odd sleep-cycle activity |\n",
    "| `day_of_week` | 1-7         | Friday-night / Sunday-evening anomalies     |\n",
    "| `is_weekend`  | binary      | Simple high-impact flag                     |\n",
    "| `month`       | 1-12        | Seasonality (Black-Friday, holidays)        |\n",
    "\n",
    "\n",
    "These velocity and burstiness indicators often separate fraud rings from normal users far better than static attributes (merchant, device…). Adding them early means they can be indexed/ cached once and reused by every downstream model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574ef5ca-ed78-47ea-942f-80e81863206a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+-------+-------+-------+-----------+------------------+\n|sender_account|event_time                |amount |txns_1h|amt_24h|uniq_rcv_7d|avg_gap_mins_10   |\n+--------------+--------------------------+-------+-------+-------+-----------+------------------+\n|ACC100003     |2023-04-13 14:38:00.52001 |16.19  |0      |0.0    |0          |9999.0            |\n|ACC100003     |2023-04-30 03:53:31.626724|1200.48|0      |0.0    |0          |9999.0            |\n|ACC100003     |2023-05-21 00:57:48.160305|376.23 |0      |0.0    |0          |23835.516666666666|\n|ACC100003     |2023-08-25 17:15:09.84297 |242.04 |0      |0.0    |0          |26949.9           |\n|ACC100003     |2023-09-03 12:28:39.396761|925.5  |0      |0.0    |0          |64372.38333333334 |\n+--------------+--------------------------+-------+-------+-------+-----------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Prepare a UNIX-seconds column for easy range windows\n",
    "df_ts = df_feat.withColumn(\"ts_sec\", unix_timestamp(\"event_time\"))\n",
    "\n",
    "# Define windows (all ordered by event_time within each sender)\n",
    "w_1h  = Window.partitionBy(\"sender_account\").orderBy(\"ts_sec\") \\\n",
    "              .rangeBetween(-3600, -1)\n",
    "w_24h = Window.partitionBy(\"sender_account\").orderBy(\"ts_sec\") \\\n",
    "              .rangeBetween(-86400, -1)\n",
    "w_7d  = Window.partitionBy(\"sender_account\").orderBy(\"ts_sec\") \\\n",
    "              .rangeBetween(-86400*7, -1)\n",
    "w_last10 = Window.partitionBy(\"sender_account\").orderBy(\"ts_sec\") \\\n",
    "                 .rowsBetween(-10, -1)\n",
    "\n",
    "# Add rolling columns\n",
    "df_roll = (df_ts\n",
    "  .withColumn(\"txns_1h\",  count(\"*\").over(w_1h))\n",
    "  .withColumn(\"amt_24h\",  _sum(\"amount\").over(w_24h))\n",
    "  .withColumn(\"uniq_rcv_7d\", approx_count_distinct(\"receiver_account\").over(w_7d))\n",
    "  # gap mins: difference between successive txns\n",
    "  .withColumn(\"prev_ts\",  lag(\"ts_sec\", 1).over(Window.partitionBy(\"sender_account\")\n",
    "                                               .orderBy(\"ts_sec\")))\n",
    "  .withColumn(\"gap_mins\", (col(\"ts_sec\") - col(\"prev_ts\"))/60.0)\n",
    "  .withColumn(\"avg_gap_mins_10\", avg(\"gap_mins\").over(w_last10))\n",
    "  # zero-fill NULLs on first transactions\n",
    "  .na.fill({\"txns_1h\": 0, \"amt_24h\": 0.0, \"uniq_rcv_7d\": 0,\n",
    "            \"avg_gap_mins_10\": 9999.0})\n",
    "  .drop(\"prev_ts\", \"gap_mins\", \"ts_sec\")\n",
    ")\n",
    "\n",
    "df_roll.select(\"sender_account\", \"event_time\", \"amount\",\n",
    "               \"txns_1h\", \"amt_24h\", \"uniq_rcv_7d\",\n",
    "               \"avg_gap_mins_10\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2c344c-0854-4646-8d0d-877485218a90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7. Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5121903a-a0e3-488a-b7cd-9fb1ed67fc7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>transaction_id</th><th>timestamp</th><th>sender_account</th><th>receiver_account</th><th>amount</th><th>transaction_type</th><th>merchant_category</th><th>location</th><th>device_used</th><th>is_fraud</th><th>time_since_last_transaction</th><th>spending_deviation_score</th><th>velocity_score</th><th>geo_anomaly_score</th><th>payment_channel</th><th>ip_address</th><th>device_hash</th><th>event_time</th><th>time_since_last_tx_tmp</th><th>time_since_last_transaction_imp</th><th>hour_of_day</th><th>day_of_week</th><th>is_weekend</th><th>month</th><th>txns_1h</th><th>amt_24h</th><th>uniq_rcv_7d</th><th>avg_gap_mins_10</th></tr></thead><tbody><tr><td>T1170676</td><td>2023-04-13T14:38:00.520+0000</td><td>ACC100003</td><td>ACC195443</td><td>16.19</td><td>transfer</td><td>grocery</td><td>New York</td><td>mobile</td><td>false</td><td>-3218.619256377778</td><td>0.12</td><td>1.0</td><td>0.1</td><td>wire_transfer</td><td>184.22.170.215</td><td>D8945075</td><td>2023-04-13T14:38:00.520+0000</td><td>3218.619256377778</td><td>3218.619256377778</td><td>14</td><td>5</td><td>0</td><td>4</td><td>0</td><td>0.0</td><td>0</td><td>9999.0</td></tr><tr><td>T1003981</td><td>2023-04-30T03:53:31.626+0000</td><td>ACC100003</td><td>ACC603214</td><td>1200.48</td><td>deposit</td><td>restaurant</td><td>Sydney</td><td>pos</td><td>false</td><td>null</td><td>0.89</td><td>14.0</td><td>0.37</td><td>UPI</td><td>4.177.7.251</td><td>D3263783</td><td>2023-04-30T03:53:31.626+0000</td><td>null</td><td>2566.543702108889</td><td>3</td><td>1</td><td>1</td><td>4</td><td>0</td><td>0.0</td><td>0</td><td>9999.0</td></tr><tr><td>T3303332</td><td>2023-05-21T00:57:48.160+0000</td><td>ACC100003</td><td>ACC635752</td><td>376.23</td><td>withdrawal</td><td>retail</td><td>London</td><td>web</td><td>false</td><td>-5164.552279159444</td><td>0.26</td><td>17.0</td><td>0.59</td><td>UPI</td><td>65.2.227.169</td><td>D9077572</td><td>2023-05-21T00:57:48.160+0000</td><td>5164.552279159444</td><td>5164.552279159444</td><td>0</td><td>1</td><td>1</td><td>5</td><td>0</td><td>0.0</td><td>0</td><td>23835.516666666666</td></tr><tr><td>T1149234</td><td>2023-08-25T17:15:09.842+0000</td><td>ACC100003</td><td>ACC107883</td><td>242.04</td><td>payment</td><td>retail</td><td>New York</td><td>mobile</td><td>false</td><td>-211.22487605305557</td><td>0.37</td><td>16.0</td><td>0.06</td><td>UPI</td><td>137.154.146.174</td><td>D2046783</td><td>2023-08-25T17:15:09.842+0000</td><td>211.22487605305557</td><td>211.22487605305557</td><td>17</td><td>6</td><td>0</td><td>8</td><td>0</td><td>0.0</td><td>0</td><td>26949.9</td></tr><tr><td>T1018914</td><td>2023-09-03T12:28:39.396+0000</td><td>ACC100003</td><td>ACC204438</td><td>925.5</td><td>deposit</td><td>other</td><td>Berlin</td><td>mobile</td><td>false</td><td>3032.585491676944</td><td>-0.4</td><td>16.0</td><td>0.38</td><td>UPI</td><td>13.68.170.193</td><td>D3989703</td><td>2023-09-03T12:28:39.396+0000</td><td>3032.585491676944</td><td>3032.585491676944</td><td>12</td><td>1</td><td>1</td><td>9</td><td>0</td><td>0.0</td><td>0</td><td>64372.38333333334</td></tr><tr><td>T4836104</td><td>2023-12-04T04:56:39.076+0000</td><td>ACC100003</td><td>ACC316067</td><td>87.02</td><td>payment</td><td>restaurant</td><td>Singapore</td><td>web</td><td>false</td><td>4731.980809944166</td><td>-0.82</td><td>19.0</td><td>0.39</td><td>ACH</td><td>172.183.210.108</td><td>D7287276</td><td>2023-12-04T04:56:39.076+0000</td><td>4731.980809944166</td><td>4731.980809944166</td><td>4</td><td>2</td><td>0</td><td>12</td><td>0</td><td>0.0</td><td>0</td><td>51447.662500000006</td></tr><tr><td>T1997184</td><td>2023-12-22T05:30:56.365+0000</td><td>ACC100003</td><td>ACC672980</td><td>90.72</td><td>payment</td><td>restaurant</td><td>Berlin</td><td>mobile</td><td>false</td><td>6062.882179241388</td><td>-1.25</td><td>14.0</td><td>0.74</td><td>card</td><td>18.59.0.96</td><td>D5812741</td><td>2023-12-22T05:30:56.365+0000</td><td>6062.882179241388</td><td>6062.882179241388</td><td>5</td><td>6</td><td>0</td><td>12</td><td>0</td><td>0.0</td><td>0</td><td>67563.73000000001</td></tr><tr><td>T4421291</td><td>2023-01-05T18:16:39.685+0000</td><td>ACC100016</td><td>ACC292179</td><td>15.02</td><td>transfer</td><td>grocery</td><td>Sydney</td><td>mobile</td><td>false</td><td>-6563.549407569167</td><td>1.11</td><td>4.0</td><td>0.87</td><td>ACH</td><td>13.244.135.228</td><td>D1144030</td><td>2023-01-05T18:16:39.685+0000</td><td>6563.549407569167</td><td>6563.549407569167</td><td>18</td><td>5</td><td>0</td><td>1</td><td>0</td><td>0.0</td><td>0</td><td>9999.0</td></tr><tr><td>T3674154</td><td>2023-10-06T05:49:37.552+0000</td><td>ACC100016</td><td>ACC324167</td><td>112.71</td><td>withdrawal</td><td>travel</td><td>London</td><td>pos</td><td>false</td><td>-495.2550474727778</td><td>0.43</td><td>6.0</td><td>0.52</td><td>wire_transfer</td><td>201.241.136.239</td><td>D6627996</td><td>2023-10-06T05:49:37.552+0000</td><td>495.2550474727778</td><td>495.2550474727778</td><td>5</td><td>6</td><td>0</td><td>10</td><td>0</td><td>0.0</td><td>0</td><td>9999.0</td></tr><tr><td>T1674210</td><td>2023-10-26T21:04:55.723+0000</td><td>ACC100016</td><td>ACC862571</td><td>221.71</td><td>withdrawal</td><td>other</td><td>Berlin</td><td>pos</td><td>false</td><td>null</td><td>-0.04</td><td>18.0</td><td>0.35</td><td>ACH</td><td>7.181.248.86</td><td>D9962992</td><td>2023-10-26T21:04:55.723+0000</td><td>null</td><td>2566.543702108889</td><td>21</td><td>5</td><td>0</td><td>10</td><td>0</td><td>0.0</td><td>0</td><td>393812.9666666667</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "T1170676",
         "2023-04-13T14:38:00.520+0000",
         "ACC100003",
         "ACC195443",
         16.19,
         "transfer",
         "grocery",
         "New York",
         "mobile",
         false,
         -3218.619256377778,
         0.12,
         1.0,
         0.1,
         "wire_transfer",
         "184.22.170.215",
         "D8945075",
         "2023-04-13T14:38:00.520+0000",
         3218.619256377778,
         3218.619256377778,
         14,
         5,
         0,
         4,
         0,
         0.0,
         0,
         9999.0
        ],
        [
         "T1003981",
         "2023-04-30T03:53:31.626+0000",
         "ACC100003",
         "ACC603214",
         1200.48,
         "deposit",
         "restaurant",
         "Sydney",
         "pos",
         false,
         null,
         0.89,
         14.0,
         0.37,
         "UPI",
         "4.177.7.251",
         "D3263783",
         "2023-04-30T03:53:31.626+0000",
         null,
         2566.543702108889,
         3,
         1,
         1,
         4,
         0,
         0.0,
         0,
         9999.0
        ],
        [
         "T3303332",
         "2023-05-21T00:57:48.160+0000",
         "ACC100003",
         "ACC635752",
         376.23,
         "withdrawal",
         "retail",
         "London",
         "web",
         false,
         -5164.552279159444,
         0.26,
         17.0,
         0.59,
         "UPI",
         "65.2.227.169",
         "D9077572",
         "2023-05-21T00:57:48.160+0000",
         5164.552279159444,
         5164.552279159444,
         0,
         1,
         1,
         5,
         0,
         0.0,
         0,
         23835.516666666666
        ],
        [
         "T1149234",
         "2023-08-25T17:15:09.842+0000",
         "ACC100003",
         "ACC107883",
         242.04,
         "payment",
         "retail",
         "New York",
         "mobile",
         false,
         -211.22487605305557,
         0.37,
         16.0,
         0.06,
         "UPI",
         "137.154.146.174",
         "D2046783",
         "2023-08-25T17:15:09.842+0000",
         211.22487605305557,
         211.22487605305557,
         17,
         6,
         0,
         8,
         0,
         0.0,
         0,
         26949.9
        ],
        [
         "T1018914",
         "2023-09-03T12:28:39.396+0000",
         "ACC100003",
         "ACC204438",
         925.5,
         "deposit",
         "other",
         "Berlin",
         "mobile",
         false,
         3032.585491676944,
         -0.4,
         16.0,
         0.38,
         "UPI",
         "13.68.170.193",
         "D3989703",
         "2023-09-03T12:28:39.396+0000",
         3032.585491676944,
         3032.585491676944,
         12,
         1,
         1,
         9,
         0,
         0.0,
         0,
         64372.38333333334
        ],
        [
         "T4836104",
         "2023-12-04T04:56:39.076+0000",
         "ACC100003",
         "ACC316067",
         87.02,
         "payment",
         "restaurant",
         "Singapore",
         "web",
         false,
         4731.980809944166,
         -0.82,
         19.0,
         0.39,
         "ACH",
         "172.183.210.108",
         "D7287276",
         "2023-12-04T04:56:39.076+0000",
         4731.980809944166,
         4731.980809944166,
         4,
         2,
         0,
         12,
         0,
         0.0,
         0,
         51447.662500000006
        ],
        [
         "T1997184",
         "2023-12-22T05:30:56.365+0000",
         "ACC100003",
         "ACC672980",
         90.72,
         "payment",
         "restaurant",
         "Berlin",
         "mobile",
         false,
         6062.882179241388,
         -1.25,
         14.0,
         0.74,
         "card",
         "18.59.0.96",
         "D5812741",
         "2023-12-22T05:30:56.365+0000",
         6062.882179241388,
         6062.882179241388,
         5,
         6,
         0,
         12,
         0,
         0.0,
         0,
         67563.73000000001
        ],
        [
         "T4421291",
         "2023-01-05T18:16:39.685+0000",
         "ACC100016",
         "ACC292179",
         15.02,
         "transfer",
         "grocery",
         "Sydney",
         "mobile",
         false,
         -6563.549407569167,
         1.11,
         4.0,
         0.87,
         "ACH",
         "13.244.135.228",
         "D1144030",
         "2023-01-05T18:16:39.685+0000",
         6563.549407569167,
         6563.549407569167,
         18,
         5,
         0,
         1,
         0,
         0.0,
         0,
         9999.0
        ],
        [
         "T3674154",
         "2023-10-06T05:49:37.552+0000",
         "ACC100016",
         "ACC324167",
         112.71,
         "withdrawal",
         "travel",
         "London",
         "pos",
         false,
         -495.2550474727778,
         0.43,
         6.0,
         0.52,
         "wire_transfer",
         "201.241.136.239",
         "D6627996",
         "2023-10-06T05:49:37.552+0000",
         495.2550474727778,
         495.2550474727778,
         5,
         6,
         0,
         10,
         0,
         0.0,
         0,
         9999.0
        ],
        [
         "T1674210",
         "2023-10-26T21:04:55.723+0000",
         "ACC100016",
         "ACC862571",
         221.71,
         "withdrawal",
         "other",
         "Berlin",
         "pos",
         false,
         null,
         -0.04,
         18.0,
         0.35,
         "ACH",
         "7.181.248.86",
         "D9962992",
         "2023-10-26T21:04:55.723+0000",
         null,
         2566.543702108889,
         21,
         5,
         0,
         10,
         0,
         0.0,
         0,
         393812.9666666667
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "transaction_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "sender_account",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "receiver_account",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "transaction_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "merchant_category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "device_used",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "is_fraud",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "time_since_last_transaction",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "spending_deviation_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "velocity_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "geo_anomaly_score",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "payment_channel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "ip_address",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "device_hash",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "event_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "time_since_last_tx_tmp",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "time_since_last_transaction_imp",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "hour_of_day",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "day_of_week",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "is_weekend",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "month",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "txns_1h",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "amt_24h",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "uniq_rcv_7d",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "avg_gap_mins_10",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_roll.limit(10).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d3009da-b8f7-4947-b1e2-1bf7b571717a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- transaction_id: string (nullable = true)\n |-- timestamp: timestamp (nullable = true)\n |-- sender_account: string (nullable = true)\n |-- receiver_account: string (nullable = true)\n |-- amount: double (nullable = true)\n |-- transaction_type: string (nullable = true)\n |-- merchant_category: string (nullable = true)\n |-- location: string (nullable = true)\n |-- device_used: string (nullable = true)\n |-- is_fraud: boolean (nullable = true)\n |-- time_since_last_transaction: double (nullable = true)\n |-- spending_deviation_score: double (nullable = true)\n |-- velocity_score: double (nullable = true)\n |-- geo_anomaly_score: double (nullable = true)\n |-- payment_channel: string (nullable = true)\n |-- ip_address: string (nullable = true)\n |-- device_hash: string (nullable = true)\n |-- event_time: timestamp (nullable = true)\n |-- time_since_last_tx_tmp: double (nullable = true)\n |-- time_since_last_transaction_imp: double (nullable = true)\n |-- hour_of_day: integer (nullable = true)\n |-- day_of_week: integer (nullable = true)\n |-- is_weekend: integer (nullable = false)\n |-- month: integer (nullable = true)\n |-- txns_1h: long (nullable = false)\n |-- amt_24h: double (nullable = false)\n |-- uniq_rcv_7d: long (nullable = false)\n |-- avg_gap_mins_10: double (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "df_roll.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f21f0bc6-76e4-4e17-a92c-3fbe0476438a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 7.1 Define final variable columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "332ff745-62e2-4ad7-8d05-12033c06849d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Numeric feature columns\n",
    "num_cols = [\n",
    "    # “raw” numeric signals\n",
    "    \"amount\",\n",
    "    \"spending_deviation_score\",\n",
    "    \"velocity_score\",\n",
    "    \"geo_anomaly_score\",\n",
    "    \"time_since_last_transaction_imp\",\n",
    "    # rolling / behavioural features\n",
    "    \"txns_1h\",\n",
    "    \"amt_24h\",\n",
    "    \"uniq_rcv_7d\",\n",
    "    \"avg_gap_mins_10\",\n",
    "]\n",
    "\n",
    "# Low-cardinality categoricals\n",
    "cat_cols = [\n",
    "    # business / transaction attrs\n",
    "    \"transaction_type\",\n",
    "    \"merchant_category\",\n",
    "    \"location\",\n",
    "    \"device_used\",\n",
    "    \"payment_channel\",\n",
    "    # calendar attrs\n",
    "    \"hour_of_day\",\n",
    "    \"day_of_week\",\n",
    "    \"is_weekend\",\n",
    "    \"month\",\n",
    "]\n",
    "\n",
    "# High-cardinality IDs\n",
    "# sender_account, receiver_account  → frequency-encode or hash\n",
    "# ip_address, device_hash           → hash (/24 bucket) or drop\n",
    "high_card_ids = [\"sender_account\", \"receiver_account\",\n",
    "                 \"ip_address\", \"device_hash\"]\n",
    "\n",
    "# Pure primary-key column to drop\n",
    "drop_cols = [\"transaction_id\",\n",
    "             \"time_since_last_transaction\",  # raw & superseded\n",
    "             \"time_since_last_tx_tmp\",       # temp column\n",
    "             \"timestamp\", \"event_time\"]      # keep only if you need in inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92178ab3-e9bb-40f7-8771-24223bef48fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 7.2 Create log transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39e29c50-6f0e-4b2d-bed0-99fc664f3e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To improve model performance and numerical stability, we apply a log-transformation to skewed numeric features—specifically `amount` and `time_since_last_transaction_imp`. These variables have long-tailed distributions that can negatively impact the learning of certain model types.\n",
    "\n",
    "We use `SQLTransformer` to apply `log1p(x)` (which computes `log(1 + x)`) and create the following new features:\n",
    "\n",
    "- `log_amount`  \n",
    "- `log_time_since_last_tx`\n",
    "\n",
    "These are added **in addition to** the original features when training tree-based models such as Gradient-Boosted Trees, which are generally robust to skew and may benefit from both representations.\n",
    "\n",
    "For **linear or distance-based models** (e.g., logistic regression, k-NN), which are more sensitive to feature scaling and skewness, we create an alternate numeric column set that **excludes** the original `amount` and `time_since_last_transaction_imp`, replacing them with their log-transformed counterparts:\n",
    "\n",
    "```python\n",
    "num_cols_ext_linear_base_models = num_cols_mod + [\"log_amount\", \"log_time_since_last_tx\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fe10dad-cb00-434e-9bac-54e6db89e140",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "log_amount_sql = SQLTransformer(\n",
    "    statement=\"SELECT *, log1p(amount) AS log_amount FROM __THIS__\"\n",
    ")\n",
    "\n",
    "log_ts_sql = SQLTransformer(\n",
    "    statement=\"SELECT *, log1p(time_since_last_transaction_imp) AS log_time_since_last_tx FROM __THIS__\"\n",
    ")\n",
    "\n",
    "num_cols_ext = num_cols + [\"log_amount\", \"log_time_since_last_tx\"] \n",
    "\n",
    "\n",
    "# For linear/ distance-based models\n",
    "num_cols_mod = [c for c in num_cols\n",
    "                if c not in (\"amount\", \"time_since_last_transaction_imp\")]\n",
    "num_cols_ext_linear_base_models = num_cols_mod + [\"log_amount\", \"log_time_since_last_tx\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39135024-feaa-412f-b1e9-b0d5900d063f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 7.3 Low cardinality categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78a379e-9ecf-4799-be04-b041f444555c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  \n",
    "To convert categorical features into a numerical format suitable for machine learning algorithms, we apply a two-step encoding process:\n",
    "\n",
    "1. **`StringIndexer`**: Converts each categorical value into a numeric index. This is necessary because most ML algorithms cannot directly handle string-type inputs.\n",
    "\n",
    "2. **`OneHotEncoder`**: Transforms indexed categories into sparse binary vectors. For example, a feature with 4 categories will be represented as a 4-dimensional vector where only one element is `1.0` and the rest are `0.0`.\n",
    "\n",
    "We configure:\n",
    "\n",
    "- `handleInvalid=\"keep\"` in `StringIndexer` to ensure that any unseen category in validation/test data is assigned a default bucket.\n",
    "- `dropLast=False` in `OneHotEncoder` to retain all categories (important for tree-based models that can handle redundancy well).\n",
    "\n",
    "This encoding process is applied to all features in the `cat_cols` list, and the resulting columns (e.g., `device_type_ohe`, `payment_channel_ohe`, etc.) are stored in `ohe_cols` for later use in the `VectorAssembler`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0601e33f-52eb-447e-8002-3d119b83be19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "indexers = [StringIndexer(inputCol=c,\n",
    "                          outputCol=f\"{c}_idx\",\n",
    "                          handleInvalid=\"keep\")\n",
    "            for c in cat_cols]\n",
    "\n",
    "encoders = [OneHotEncoder(inputCol=idx.getOutputCol(),\n",
    "                          outputCol=f\"{idx.getOutputCol()}_ohe\",\n",
    "                          dropLast=False)\n",
    "            for idx in indexers]\n",
    "\n",
    "ohe_cols = [e.getOutputCol() for e in encoders]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b89c716b-6dee-4420-a788-b839907d9c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 7.4 High cardnality IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0565c835-a441-4ed0-b48e-28628d6bc8c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Some categorical features, such as `sender_account` and `receiver_account`, have **very high cardinality** (hundreds of thousands of unique values). Applying traditional one-hot encoding to such features would result in extremely sparse and wide feature vectors, which is computationally expensive and prone to overfitting.\n",
    "\n",
    "To address this, we use **`FeatureHasher`** to convert high-cardinality string features into fixed-length numerical vectors. This approach:\n",
    "\n",
    "- Projects the categorical input into a lower-dimensional hashed feature space.\n",
    "- Retains some of the identity signal while keeping the feature space compact.\n",
    "- Avoids the explosion of dimensions caused by one-hot encoding.\n",
    "\n",
    "In this project, we hash both `sender_account` and `receiver_account` into 1,024-dimensional vectors using the following configuration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dc9d755-97fb-4be7-96bb-282d07dc07d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hasher_sender = FeatureHasher(inputCols=[\"sender_account\"],\n",
    "                              outputCol=\"sender_hash\",\n",
    "                              numFeatures=1024)\n",
    "hasher_receiver = FeatureHasher(inputCols=[\"receiver_account\"],\n",
    "                                outputCol=\"receiver_hash\",\n",
    "                                numFeatures=1024)\n",
    "\n",
    "num_cols_mod = num_cols_mod + [\"sender_hash\", \"receiver_hash\"]\n",
    "\n",
    "num_cols_ext_linear_base_models = num_cols_ext_linear_base_models + [\"sender_hash\", \"receiver_hash\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "370bd64a-99ca-43bd-b225-8313dfae0179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 7.5 VectorAssembler -> StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60ef98eb-dcee-41c4-924a-fbce39437567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "After applying various transformations to numeric, categorical, and high-cardinality features, we need to **consolidate all input features into a single vector column** for model consumption. This is done using two final steps:\n",
    "\n",
    "1. **`VectorAssembler`**: Combines all selected features—numerical, log-transformed, hashed IDs, and one-hot encoded categoricals—into a single vector column called `features_raw`.\n",
    "\n",
    "2. **`StandardScaler`**: Normalizes the `features_raw` vector by scaling each feature to have unit standard deviation. This is particularly useful for models that are sensitive to feature magnitude (e.g., logistic regression, neural networks). Although tree-based models are usually invariant to scale, applying `StandardScaler` ensures compatibility and consistency across different model types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd3547b9-e267-4cdb-ab18-7c3b74ed173e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=num_cols_ext + [\"sender_hash\", \"receiver_hash\"] + ohe_cols,\n",
    "    outputCol=\"features_raw\"\n",
    ")\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features_raw\",\n",
    "    outputCol=\"features\",\n",
    "    withStd=True, withMean=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d13213e-2f97-42ed-80e3-4bcd2b9365b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 7.6 Label and weight columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "034ef8d3-9267-469d-a50c-fab38b2b1ef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "In order to train supervised machine learning models in Spark, we must provide two special columns:\n",
    "\n",
    "- `label`: the target column representing the outcome to predict (in our case, whether a transaction is fraudulent). Spark ML requires this column to be of type `double`.\n",
    "- `weight`: an optional column used to give **more importance to rare classes** during training. This is especially important in imbalanced classification tasks like fraud detection.\n",
    "\n",
    "Since fraud occurs in roughly **3–4% of transactions**, we compute a **class weight ratio of 26:1** and assign:\n",
    "\n",
    "- `weight = 26.0` to fraud rows (`is_fraud = True`)\n",
    "- `weight = 1.0` to non-fraud rows (`is_fraud = False`)\n",
    "\n",
    "This helps the model treat the minority class more seriously during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de6d3424-d65d-41a2-9247-02f6c0adf41c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_final = df_roll.withColumn(\"label\", col(\"is_fraud\").cast(\"double\")) \\\n",
    "                  .withColumn(\"weight\",\n",
    "                              when(col(\"is_fraud\"), 26.0).otherwise(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de248e09-e48c-477b-8665-ef16e71bd023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 7.7 Assembling the Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8891d24d-01c3-4e10-a464-d8290316cc4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "To ensure consistency and reproducibility across training, validation, and test datasets, we consolidate all preprocessing steps into a single Spark ML `Pipeline`. This approach guarantees that the exact same transformations are applied to any incoming data, both during model development and in production.\n",
    "\n",
    "The pipeline includes the following stages, in order:\n",
    "\n",
    "1. **Log-transforms** for skewed numeric features (`log_amount`, `log_time_since_last_tx`)\n",
    "2. **Feature hashing** for high-cardinality categorical variables (`sender_account`, `receiver_account`)\n",
    "3. **String indexing and one-hot encoding** for low-cardinality categorical variables\n",
    "4. **Vector assembly** to combine all features into a single `features_raw` column\n",
    "5. **Standard scaling** to normalize the `features_raw` vector into a final `features` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c39d3c38-5f29-492c-af45-6f9406ff9cba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stages = [\n",
    "    log_amount_sql,\n",
    "    log_ts_sql,\n",
    "    hasher_sender,\n",
    "    hasher_receiver,\n",
    "] + indexers + encoders + [\n",
    "    assembler,\n",
    "    scaler\n",
    "]\n",
    "\n",
    "full_pipeline = Pipeline(stages=stages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffa6b687-3158-4b94-9a0e-64a153f5ee7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 7.8 Time-based Train / Val / Test split\n",
    "Because our data span March → October 2023, split chronologically so the model is evaluated on future behaviour:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb2c52fa-26c8-458c-b088-c937898896f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 7.8.1 Why a chrono-split?\n",
    "\n",
    "- **Goal:** mimic real-world deployment by training on the past and validating on unseen future data.\n",
    "\n",
    "- **Ratios:** 70 % / 15 % / 15 % chosen to balance training volume with robust validation.\n",
    "\n",
    "- **Method:** find the dates where the cumulative row count reaches 70 % and 85 %.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8cedfbe-b7db-4f42-b197-4c66e3dad41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 7.8.2 Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e8a46b2-e2e8-4df7-840f-c5f88f4bbac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def find_chrono_split(df,\n",
    "                      ts_col=\"event_time\",\n",
    "                      train_frac=0.70,\n",
    "                      val_frac=0.15):\n",
    "    \"\"\"\n",
    "    Inspect df[ts_col] and return 3 boundary dates such that\n",
    "    rows <  train_cut     ≈ train_frac\n",
    "    train_cut ≤ rows < val_cut ≈ val_frac\n",
    "    rows ≥ val_cut        ≈ 1 - train_frac - val_frac   (test set)\n",
    "\n",
    "    Returns (train_cut_date, val_cut_date, counts_dict)\n",
    "    \"\"\"\n",
    "\n",
    "    # Collapse to one row per day\n",
    "    by_day = (df.withColumn(\"day\", to_date(col(ts_col)))\n",
    "                .groupBy(\"day\")\n",
    "                .agg(count(\"*\").alias(\"rows\"))\n",
    "                .orderBy(\"day\"))\n",
    "\n",
    "    total_rows = by_day.agg(_sum(\"rows\")).first()[0]\n",
    "\n",
    "    # Cumulative % over time\n",
    "    w = Window.orderBy(\"day\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "    by_day = (by_day\n",
    "              .withColumn(\"cum_rows\", _sum(\"rows\").over(w))\n",
    "              .withColumn(\"cum_pct\",  col(\"cum_rows\") / lit(total_rows)))\n",
    "\n",
    "    # Pick first day where cum_pct >= thresholds\n",
    "    train_cut  = (by_day.filter(col(\"cum_pct\") >= train_frac)\n",
    "                          .select(\"day\")\n",
    "                          .first()[0])\n",
    "    val_cut    = (by_day.filter(col(\"cum_pct\") >= train_frac + val_frac)\n",
    "                          .select(\"day\")\n",
    "                          .first()[0])\n",
    "\n",
    "    # Quick sanity counts\n",
    "    train_cnt = df.filter(col(ts_col) <  train_cut).count()\n",
    "    val_cnt   = df.filter((col(ts_col) >= train_cut) & (col(ts_col) < val_cut)).count()\n",
    "    test_cnt  = df.filter(col(ts_col) >= val_cut).count()\n",
    "\n",
    "    counts = {\"train\": train_cnt, \"val\": val_cnt, \"test\": test_cnt,\n",
    "              \"total\": total_rows}\n",
    "\n",
    "    return train_cut, val_cut, counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a10befa-d35b-4bb2-87af-348dd80a5360",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 7.8.3 Compute split boundaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e18ad40f-9f19-4201-94e9-f1d2489ba8f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic split boundaries:\n  • train <  2023-09-14\n  • val   : [ 2023-09-14 , 2023-11-07 )\n  • test  ≥ 2023-11-07\nRow counts: {'train': 3499548, 'val': 740619, 'test': 759833, 'total': 5000000}\n"
     ]
    }
   ],
   "source": [
    "train_cut, val_cut, counts = find_chrono_split(df_final)\n",
    "\n",
    "print(\"Automatic split boundaries:\")\n",
    "print(\"  • train < \", train_cut)\n",
    "print(\"  • val   : [\", train_cut, \",\", val_cut, \")\")\n",
    "print(\"  • test  ≥\", val_cut)\n",
    "print(\"Row counts:\", counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0bc78a8-5234-461d-82f1-39e85abf2591",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 7.8.4 Materialising train_df, val_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e208ffcb-7239-4a1e-b167-2d4c36549981",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rows : 3499548\nVal rows   : 740619\nTest rows  : 759833\n"
     ]
    }
   ],
   "source": [
    "train_df = df_final.filter(col(\"event_time\") <  train_cut)\n",
    "val_df   = df_final.filter((col(\"event_time\") >= train_cut) &\n",
    "                           (col(\"event_time\") <  val_cut))\n",
    "test_df  = df_final.filter(col(\"event_time\") >= val_cut)\n",
    "\n",
    "print(\"Train rows :\", train_df.count())\n",
    "print(\"Val rows   :\", val_df.count())\n",
    "print(\"Test rows  :\", test_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18eb01e8-3f47-469e-8b76-b24d522f89e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 7.8.5 Quick leakage sanity-check\n",
    "\n",
    "Verify no sender accounts appear for the first time in validation or test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "724a443a-8be1-4880-9e9e-171ab56db5a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brand-new accounts in val set: 15287\n"
     ]
    }
   ],
   "source": [
    "first_seen_train = train_df.select(\"sender_account\").distinct()\n",
    "val_new_accounts = (val_df\n",
    "                    .join(first_seen_train, \"sender_account\", \"left_anti\")\n",
    "                    .count())\n",
    "\n",
    "print(\"Brand-new accounts in val set:\", val_new_accounts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50411de6-ede1-492b-b98f-a8a5b26b8f70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "How many transactions involve these 15287 new sender?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4a921ae-ed1a-4534-bf52-e15409f42299",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transactions from brand-new accounts: 15287\nShare of val rows: 2.06%\n"
     ]
    }
   ],
   "source": [
    "val_cold_rows = (val_df\n",
    "                 .join(first_seen_train, \"sender_account\", \"left_anti\")\n",
    "                 .count())\n",
    "print(\"Transactions from brand-new accounts:\", val_cold_rows)\n",
    "print(f\"Share of val rows: {val_cold_rows/740_619:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c28def8b-5f5f-47f5-9658-7750828614e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Fraud counts per split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1782946c-2a8a-4f5d-b63f-c207d8fe2574",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train | rows = 3,499,548  | fraud % =  3.58\nVal   | rows = 740,619  | fraud % =  3.60\nTest  | rows = 759,833  | fraud % =  3.61\n"
     ]
    }
   ],
   "source": [
    "def fraud_share(df, name):\n",
    "    total  = df.count()\n",
    "    fraud  = df.filter(\"is_fraud\").count()\n",
    "    print(f\"{name:<5} | rows = {total:,}  | fraud % = {fraud/total*100:5.2f}\")\n",
    "\n",
    "for frame, label in [(train_df, \"Train\"),\n",
    "                     (val_df,   \"Val\"),\n",
    "                     (test_df,  \"Test\")]:\n",
    "    fraud_share(frame, label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c84530-75f5-4f0e-bc81-b5b1af6e151e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d009013-5865-4583-9f16-b4b8119d8f24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8. Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af286e40-eedd-4f7d-af69-3393992df5f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 8.1 Prepare target and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de34a766-f678-489b-a936-79826b3c0814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cast the target → \"label\", then add the weight column\n",
    "train_df = (train_df\n",
    "            .withColumn(\"label\",  F.col(\"is_fraud\").cast(\"double\"))\n",
    "            .withColumn(\"weight\", F.when(F.col(\"is_fraud\"), 26.0)     \n",
    "                                   .otherwise(1.0)))\n",
    "\n",
    "val_df  = val_df .withColumn(\"label\", F.col(\"is_fraud\").cast(\"double\"))\n",
    "test_df = test_df.withColumn(\"label\", F.col(\"is_fraud\").cast(\"double\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2707f8f-906d-4d48-a4fe-019ee9546ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Why 26 ? -> from our quick stats, fraud ≈ 3.8 % → ratio ≈ 1 : 26.\n",
    "Giving minority rows a weight equal to that ratio makes the loss treat both classes roughly equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0024d4f3-fdde-4865-871a-b0815f8e5b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 8.2 Fit the pre-processing pipeline and transform every split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17672430-2f60-4253-86d3-f6daee476c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train schema:\nroot\n |-- features: vector (nullable = true)\n |-- label: double (nullable = true)\n |-- weight: double (nullable = false)\n\nTrain rows: 3499548   Val rows: 740619   Test rows: 759833\n"
     ]
    }
   ],
   "source": [
    "# Fitting the pipeline on TRAIN only\n",
    "preproc_model = full_pipeline.fit(train_df)\n",
    "\n",
    "# Transforming all three splits once and cache the results\n",
    "train_ready = preproc_model.transform(train_df).cache()\n",
    "val_ready   = preproc_model.transform(val_df).cache()\n",
    "test_ready  = preproc_model.transform(test_df).cache()\n",
    "\n",
    "# Quick sanity: making sure the vector column exists\n",
    "print(\"Train schema:\")\n",
    "train_ready.select(\"features\", \"label\", \"weight\").printSchema()\n",
    "print(\"Train rows:\", train_ready.count(),\n",
    "      \"  Val rows:\", val_ready.count(),\n",
    "      \"  Test rows:\", test_ready.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cf06d54-f329-4e26-81f5-bb2513b276a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 8.3 Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb5e3ab7-9d5a-4c45-a63a-856c12702de0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # ------------------------------------------------------------------\n",
    "# # 8.4 🔎  Feature-importance selection (index-only fallback)\n",
    "# # ------------------------------------------------------------------\n",
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "# from pyspark.ml.feature        import VectorSlicer\n",
    "# import numpy as np\n",
    "\n",
    "# # --- 1. quick Random Forest on processed data ---------------------\n",
    "# rf = RandomForestClassifier(\n",
    "#         featuresCol=\"features\",\n",
    "#         labelCol=\"label\",\n",
    "#         weightCol=\"weight\",\n",
    "#         numTrees=100,\n",
    "#         maxDepth=6,\n",
    "#         seed=42)\n",
    "\n",
    "# rf_model = rf.fit(train_ready)\n",
    "\n",
    "# # --- 2. grab importances and pick top-10 indices ------------------\n",
    "# imp = rf_model.featureImportances.toArray()\n",
    "# top_k = 10\n",
    "# top_idx = np.argsort(imp)[::-1][:top_k].tolist()\n",
    "\n",
    "# print(\"Top-10 feature indices:\", top_idx)\n",
    "\n",
    "# # --- 3. build a VectorSlicer to keep only those dimensions --------\n",
    "# from pyspark.ml.feature import VectorSlicer\n",
    "# slicer = VectorSlicer(inputCol=\"features\",\n",
    "#                       outputCol=\"features_topK\",\n",
    "#                       indices=top_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f3042f0-9b43-4cd1-a16d-ce385059fdc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# meta = train_ready.schema[\"features\"].metadata\n",
    "# attrs = []\n",
    "\n",
    "# for group in [\"numeric\", \"binary\", \"categorical\"]:\n",
    "#     if \"ml_attr\" in meta and group in meta[\"ml_attr\"].get(\"attrs\", {}):\n",
    "#         attrs += meta[\"ml_attr\"][\"attrs\"][group]\n",
    "\n",
    "# name_by_index = [attr[\"name\"] for attr in sorted(attrs, key=lambda x: x[\"idx\"])]\n",
    "\n",
    "# # Now print the top-10 features by name\n",
    "# for i in top_idx:\n",
    "#     name = name_by_index[i] if i < len(name_by_index) else f\"<hashed_{i}>\"\n",
    "#     print(f\"Index {i}: {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa76c1e3-af10-43d5-936c-c5f1eddd5008",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 8.4 Persist artefacts and exit session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "698e5f68-7915-4782-be79-8b7ab0acbb33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # 1) --- Fit and save the slicer as a pipeline --------------------\n",
    "\n",
    "# slicer_pipe = Pipeline(stages=[slicer])\n",
    "# slicer_model = slicer_pipe.fit(train_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef2a9298-0b81-4268-ac26-b4a634811b25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved preproc pipeline, fitted slicer, and ready datasets.\n"
     ]
    }
   ],
   "source": [
    "# # 1) --- Fit and save the slicer as a pipeline --------------------\n",
    "\n",
    "# slicer_pipe = Pipeline(stages=[slicer])\n",
    "# slicer_model = slicer_pipe.fit(train_ready)\n",
    "# slicer_model.write().overwrite().save(\"/FileStore/models/slicer_top10\")\n",
    "\n",
    "# 2) --- Persist preprocessing pipeline + ready data --------------\n",
    "preproc_model.write().overwrite().save(\"/FileStore/models/preproc_model\")\n",
    "\n",
    "train_ready.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/data/train_ready\")\n",
    "val_ready  .write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/data/val_ready\")\n",
    "test_ready .write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/data/test_ready\")\n",
    "\n",
    "print(\"✅ Saved preproc pipeline, fitted slicer, and ready datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e41267-d035-445e-ad4c-0eb37bba53cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[41]: [FileInfo(path='dbfs:/FileStore/models/slicer_top10/metadata/', name='metadata/', size=0, modificationTime=0),\n FileInfo(path='dbfs:/FileStore/models/slicer_top10/stages/', name='stages/', size=0, modificationTime=0)]"
     ]
    }
   ],
   "source": [
    "dbutils.fs.ls(\"/FileStore/models/slicer_top10/\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_PreProc_RL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}