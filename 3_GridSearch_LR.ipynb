{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c0fbd1e-a094-4a2b-acd7-c2db9f00cbfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Notebook 3: Train and evaluate Logistic Regression using selected features\n",
    "\n",
    "# from pyspark.ml import PipelineModel\n",
    "# from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# from pyspark.ml.classification import LogisticRegression\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# # Load slicing stage (top-10 features)\n",
    "# slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "\n",
    "# # Load preprocessed datasets\n",
    "# train_ready = spark.read.format(\"delta\").load(\"/FileStore/data/train_ready\")\n",
    "# val_ready   = spark.read.format(\"delta\").load(\"/FileStore/data/val_ready\")\n",
    "\n",
    "# # Apply slicer to keep only top-10 features\n",
    "# train_topk = slicer_model.transform(train_ready)\n",
    "# val_topk   = slicer_model.transform(val_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da6805d0-e99e-427c-a8f7-b4c4e7f35c50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Split into majority and minority classes\n",
    "# minority_df = train_topk.filter(\"label = 1\")\n",
    "# majority_df = train_topk.filter(\"label = 0\")\n",
    "\n",
    "# # Sample only majority class (e.g., keep 80%)\n",
    "# majority_sampled = majority_df.sample(withReplacement=False, fraction=0.6, seed=42)\n",
    "\n",
    "# # Combine back together\n",
    "# train_sample = minority_df.union(majority_sampled)\n",
    "\n",
    "# # Optional: Shuffle the dataset (if desired)\n",
    "# train_sample = train_sample.orderBy(F.rand(seed=42))\n",
    "\n",
    "# # Define LR\n",
    "# lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features_topK\")\n",
    "\n",
    "# # Param grid (keep it small!)\n",
    "# param_grid = ParamGridBuilder() \\\n",
    "#     .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "#     .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "#     .build()\n",
    "\n",
    "# # Evaluator\n",
    "# evaluator = MulticlassClassificationEvaluator(\n",
    "#     labelCol=\"label\", \n",
    "#     predictionCol=\"prediction\", \n",
    "#     metricName=\"f1\"\n",
    "# )\n",
    "\n",
    "# train_split, val_split = train_sample.randomSplit([0.8, 0.2], seed=42)\n",
    "# tvs = TrainValidationSplit(\n",
    "#     estimator=lr,\n",
    "#     estimatorParamMaps=param_grid,\n",
    "#     evaluator=evaluator,\n",
    "#     trainRatio=1.0,  # Porque j√° dividiste\n",
    "#     parallelism=1\n",
    "# )\n",
    "# tvs_model = tvs.fit(train_split)\n",
    "\n",
    "# # Apply best model on validation\n",
    "# val_preds = tvs_model.transform(val_topk)\n",
    "# f1_score = evaluator.evaluate(val_preds)\n",
    "\n",
    "# print(f\"Best model F1-score on validation set: {f1_score:.4f}\")\n",
    "\n",
    "# # Save the best model\n",
    "# tvs_model.bestModel.write().overwrite().save(\"/FileStore/models/lr_top10_model_grid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df8ae09-6f94-4dd9-ad7b-21d10b51190a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model F1-score on validation set: 0.9463\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Load slicing stage (top-10 features)\n",
    "slicer_model = PipelineModel.load(\"/FileStore/models/slicer_top10\")\n",
    "\n",
    "# Load preprocessed datasets\n",
    "train_ready = spark.read.format(\"delta\").load(\"/FileStore/data/train_ready\")\n",
    "val_ready   = spark.read.format(\"delta\").load(\"/FileStore/data/val_ready\")\n",
    "\n",
    "# Apply slicer to keep only top-10 features\n",
    "train_topk = slicer_model.transform(train_ready)\n",
    "val_topk   = slicer_model.transform(val_ready)\n",
    "\n",
    "# Efficient class balancing using sampleBy\n",
    "fractions = {0: 0.6, 1: 1.0}  # 60% of majority, 100% of minority\n",
    "train_sample = train_topk.sampleBy(\"label\", fractions=fractions, seed=42)\n",
    "\n",
    "# Define Logistic Regression\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features_topK\")\n",
    "\n",
    "# Small param grid to limit overhead\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Evaluator (can switch to BinaryClassificationEvaluator if binary)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "# TrainValidationSplit (no internal split since we did it manually)\n",
    "tvs = TrainValidationSplit(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=param_grid,\n",
    "    evaluator=evaluator,\n",
    "    trainRatio=1.0,\n",
    "    parallelism=1  # Required for Databricks CE\n",
    ")\n",
    "\n",
    "# Train the model with grid search\n",
    "tvs_model = tvs.fit(train_sample)\n",
    "\n",
    "# Evaluate on external validation set\n",
    "val_preds = tvs_model.transform(val_topk)\n",
    "f1_score = evaluator.evaluate(val_preds)\n",
    "\n",
    "print(f\"Best model F1-score on validation set: {f1_score:.4f}\")\n",
    "\n",
    "# Save best model\n",
    "tvs_model.bestModel.write().overwrite().save(\"/FileStore/models/lr_top10_model_grid\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook3_LR",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}